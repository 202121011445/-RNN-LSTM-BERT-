import os
import pandas as pd
from sklearn import metrics
from sklearn.metrics import f1_score, accuracy_score
import torch as d2l

from word2vec import *
import numpy as np


def truncate_pad(line, num_steps, padding_token):
    """æˆªæ–­æˆ–å¡«å……æ–‡æœ¬åºåˆ—"""
    if len(line) > num_steps:
        return line[:num_steps]  # æˆªæ–­
    return line + [padding_token] * (num_steps - len(line))  # å¡«å……


class BiRNN(nn.Module):
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, **kwargs):
        super(BiRNN, self).__init__(**kwargs)
        self.embedding = nn.Embedding(vocab_size, embed_size)
        # åŒå‘å¾ªç¯ç¥ç»ç½‘ç»œ
        # self.encoder = nn.LSTM(embed_size, num_hiddens, num_layers=num_layers, bidirectional=True)
        self.encoder = nn.RNN(embed_size, num_hiddens, num_layers=num_layers, bidirectional=True)
        self.decoder = nn.Linear(4 * num_hiddens, 1)
        self.activation = nn.Sigmoid()

    def forward(self, inputs):
        # inputsçš„å½¢çŠ¶æ˜¯ï¼ˆæ‰¹é‡å¤§å°ï¼Œæ—¶é—´æ­¥æ•°ï¼‰ï¼Œå› ä¸ºé•¿çŸ­æœŸè®°å¿†ç½‘ç»œè¦æ±‚å…¶è¾“å…¥çš„ç¬¬ä¸€ä¸ªç»´åº¦æ˜¯æ—¶é—´ç»´ï¼Œæ‰€ä»¥åœ¨è·å¾—è¯å…ƒè¡¨ç¤ºä¹‹å‰ï¼Œè¾“å…¥ä¼šè¢«è½¬ç½®ã€‚
        # è¾“å‡ºå½¢çŠ¶ä¸ºï¼ˆæ—¶é—´æ­¥æ•°ï¼Œæ‰¹é‡å¤§å°ï¼Œè¯å‘é‡ç»´åº¦ï¼‰
        embeddings = self.embedding(inputs.T)
        self.encoder.flatten_parameters()  # é‡ç½®å‚æ•°ï¼Œåœ¨ä½¿ç”¨NVDIA GPUæ—¶å¤„ç†æ›´å¿«
        # è¿”å›ä¸Šä¸€ä¸ªéšè—å±‚åœ¨ä¸åŒæ—¶é—´æ­¥çš„éšçŠ¶æ€ï¼Œ
        # outputsçš„å½¢çŠ¶æ˜¯ï¼ˆæ—¶é—´æ­¥æ•°ï¼Œæ‰¹é‡å¤§å°ï¼Œ2*éšè—å•å…ƒæ•°ï¼‰
        outputs, _ = self.encoder(embeddings)
        # è¿ç»“åˆå§‹å’Œæœ€ç»ˆæ—¶é—´æ­¥çš„éšçŠ¶æ€ï¼Œä½œä¸ºå…¨è¿æ¥å±‚çš„è¾“å…¥ï¼Œ
        # å…¶å½¢çŠ¶ä¸ºï¼ˆæ‰¹é‡å¤§å°ï¼Œ4*éšè—å•å…ƒæ•°ï¼‰
        encoding = torch.cat((outputs[0], outputs[-1]), dim=1)
        outs = self.decoder(encoding)
        return self.activation(outs)


def init_weights(m):
    """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
    if type(m) == nn.Linear:
        nn.init.xavier_uniform_(m.weight)
    if type(m) == nn.LSTM:
        for param in m._flat_weights_names:
            if "weight" in param:
                nn.init.xavier_uniform_(m._parameters[param])


def auc(pred, labels):
    """è®¡ç®—AUC"""
    fpr, tpr, thresholds = metrics.roc_curve(y_true=labels, y_score=pred)
    return metrics.auc(fpr, tpr)


def find_best_threshold(all_predictions, all_labels):
    """å¯»æ‰¾æœ€ä½³çš„åˆ†ç±»è¾¹ç•Œ, åœ¨0åˆ°1ä¹‹é—´"""
    # å±•å¹³æ‰€æœ‰çš„é¢„æµ‹ç»“æœå’Œå¯¹åº”çš„æ ‡è®°
    all_predictions = np.ravel(all_predictions)  # å°†å¤šç»´æ•°ç»„é™ä¸ºä¸€ç»´
    all_labels = np.ravel(all_labels)
    # ä»0åˆ°1ä»¥0.01ä¸ºé—´éš”å®šä¹‰99ä¸ªå¤‡é€‰é˜ˆå€¼, åˆ†åˆ«æ˜¯ä»0.01-0.99ä¹‹é—´
    thresholds = [i / 100 for i in range(100)]
    all_auc = []
    for threshold in thresholds:
        # è®¡ç®—å½“å‰é˜ˆå€¼çš„f1 score
        preds = (all_predictions >= threshold).astype(
            "int")  # (array([1, 2, 3, 4]) > 2).astype("int") = array([0, 0, 1, 1])
        tem_auc = auc(preds, all_labels)  # ä¼ å…¥çœŸå®å€¼å’Œé¢„æµ‹å€¼
        all_auc.append(tem_auc)
    # æ‰¾å‡ºå¯ä»¥ä½¿f1 socreæœ€å¤§çš„é˜ˆå€¼
    best_threshold = thresholds[int(np.argmax(np.array(all_auc)))]
    return best_threshold, max(all_auc)


# def predict_sentiment(net, vocab, text, threshold):
#     """é¢„æµ‹æƒ…æ„Ÿ"""
#     text = torch.tensor(vocab[jieba_str(text, get_stop_words())], device=try_gpu())
#     pred = net(text.reshape(1, -1)).item()
#     if pred >= threshold:
#         return pred, 1
#     else:
#         return pred, 0


def compute_loss(predictions, labels):
    # å°†é¢„æµ‹å’Œæ ‡è®°çš„ç»´åº¦å±•å¹³
    predictions = predictions.view(-1)
    labels = labels.float().view(-1)
    epsilon = 1e-8
    # äº¤å‰ç†µ
    loss = - labels * torch.log(predictions + epsilon) - (torch.tensor(1.0) - labels) * torch.log(
        torch.tensor(1.0) - predictions + epsilon)
    loss = torch.mean(loss)
    return loss


def transform_preds(preds, threshold):
    """å°†é¢„æµ‹å€¼è½¬åŒ–ä¸º0,1æ ‡ç­¾"""
    ans = []
    for pred in preds:
        if pred[0] < threshold:
            ans.append(0)
        else:
            ans.append(1)
    return ans


if __name__ == '__main__':
    # åŠ è½½è¯­æ–™
    stop_wprds = get_stop_words()
    train_tokens, train_labels = get_corpus("./corpus/train_sentiment.txt", stop_wprds)  # åˆ†è¯ä¸”å»é™¤åœç”¨è¯æ–‡æœ¬åˆ—è¡¨
    test_tokens, test_labels = get_corpus("./corpus/test_sentiment.txt", stop_wprds)

    # æ„å»ºè¯å…¸
    vocab = Vocab(train_tokens + test_tokens, min_freq=5, reserved_tokens=['<pad>'])  # æ„å»ºå­—å…¸ã€‚len(vocab):10281
    print("å­—å…¸å¤§å°{}".format(len(vocab)))
    # ç»Ÿè®¡å¤„ç†åçš„è¯­æ–™é•¿åº¦
    lens = []
    for tokens in train_tokens + test_tokens:
        lens.append(len(tokens))
    lens = sorted(lens)
    num_steps = lens[int(0.98 * len(lens))]  # åºåˆ—é•¿åº¦ã€‚å°†nums_stepsçš„é•¿åº¦è®¾ç½®ä¸º98%è¯­å¥çš„æœ€å¤§é•¿åº¦
    print('num_steps:{}, max_len:{}'.format(num_steps, len(lens)))
    # æŒ‰ç»Ÿä¸€é•¿åº¦å°†è¯­å¥å¼ é‡åŒ–
    train_features = torch.tensor([truncate_pad(vocab[line], num_steps, vocab['<pad>']) for line in train_tokens])
    test_features = torch.tensor([truncate_pad(vocab[line], num_steps, vocab['<pad>']) for line in test_tokens])

    # æ„å»ºDataLoaderã€‚è¿­ä»£å™¨æ¯æ¬¡è¿”å›ä¸€ä¸ªbatchçš„æ•°æ®
    bath_size = 1024
    train_iter = d2l.load_array((train_features, torch.tensor(train_labels)), bath_size)  # è·å–æ•°æ®è¿­ä»£å™¨batch_sizeæ˜¯32ã€‚
    test_iter = d2l.load_array((test_features, torch.tensor(test_labels)), bath_size)

    # æ„é€ æ¨¡å‹å¹¶è¿›è¡Œåˆå§‹åŒ–
    embed_size = 100
    all_best_auc = {}
    num_hiddens_list = [128,256,512]
    num_layers_list = [1,2,3]
    for num_hiddens in num_hiddens_list:
        for num_layers in num_layers_list:
            for all_epoch in range(2):
                net = BiRNN(len(vocab), embed_size, num_hiddens, num_layers)
                net.apply(init_weights)

                # åŠ è½½é¢„è®­ç»ƒçš„è¯å‘é‡æ¨¡å‹ï¼Œå¹¶é˜²æ­¢å‚æ•°æ›´æ–°
                net.embedding.load_state_dict(torch.load('Word2vec.params'))
                net.embedding.weight.requires_grad = False

                # å‚æ•°è®¾ç½®
                dynamic_lr = 0.01
                optimizer = torch.optim.Adam(net.parameters(), lr=dynamic_lr)
                device = try_gpu()
                net = net.to(device)
                epoches = 999
                patient = 4

                # ç”¨äºè®°å½•æ•°æ®çš„åˆ—è¡¨
                train_loss = []
                test_loss = []
                train_auc = []
                test_auc = []
                train_acc = []
                test_acc = []
                train_f1 = []
                test_f1 = []
                all_best_threshold = []

                # ä½¿ç”¨pandas DataFrameè¿›è¡Œæ—¥å¿—å­˜å‚¨
                # log_path = "lstm hiddens{0} layers{1} epoch{2}.pickle".format(num_hiddens, num_layers, all_epoch)
                log_path = "rnn hiddens{0} layers{1} epoch{2}.pickle".format(num_hiddens, num_layers, all_epoch)
                print(log_path)
                if os.path.isfile(log_path):
                    os.remove(log_path)
                pd.DataFrame(columns=["epoch", "train_loss", "train_auc", "train_acc",
                                      "test_loss", "test_auc", "test_acc", "time", "threshold"
                                      ]).to_pickle(log_path)

                # è®­ç»ƒ
                for epoch in range(epoches):
                    print("epoch[{}]å­¦ä¹ ç‡ä¸º{}".format(epoch, str(dynamic_lr)))
                    # åˆå§‹åŒ–è®°å½•
                    total_loss = 0
                    train_labels, train_preds = [], []
                    # è®­ç»ƒè¿­ä»£
                    for i, (features, labels) in tqdm(enumerate(train_iter), desc="train_epoch_:%d" % (epoch),
                                                      total=len(train_iter)):
                        # å‰å‘ä¼ æ’­
                        features = features.to(device)
                        labels = labels.to(device)
                        net.train()
                        pred = net(features)
                        loss = compute_loss(pred, labels)
                        # åå‘ä¼ æ’­
                        optimizer.zero_grad()
                        loss.sum().backward()
                        optimizer.step()
                        # è®°å½•æ€»çš„æŸå¤±
                        total_loss += loss.sum().item()
                        train_labels += labels
                        train_preds += pred
                    # å°†æ•°æ®ä¼ å›CPU
                    train_labels = [i.cpu().numpy().tolist() for i in train_labels]
                    train_preds = [i.detach().cpu().numpy().tolist() for i in train_preds]
                    # ä¿å­˜æ¨¡å‹
                    # torch.save(net.state_dict(),
                    #            'lstm hiddens{0} layers{1} epoch{2}.params'.format(num_hiddens, num_layers, all_epoch))
                    torch.save(net.state_dict(),
                               'rnn hiddens{0} layers{1} epoch{2}.params'.format(num_hiddens, num_layers, all_epoch))
                    # è®¡ç®—è¯„ä»·æŒ‡æ ‡
                    best_threshold, best_auc = find_best_threshold(train_preds, train_labels)
                    train_preds = transform_preds(train_preds, best_threshold)
                    train_auc.append(best_auc)
                    train_acc.append(accuracy_score(y_pred=train_preds, y_true=train_labels))
                    train_f1.append(f1_score(y_pred=train_preds, y_true=train_labels))
                    train_loss.append(total_loss / labels.shape[0])

                    # æµ‹è¯•
                    total_loss = 0
                    test_labels, test_preds = [], []
                    net.eval()
                    with torch.no_grad():
                        for features, labels in tqdm(test_iter, desc="test_epoch_:%d" % (epoch),
                                                     total=len(test_iter)):
                            features = features.to(device)
                            labels = labels.to(device)
                            pred = net(features)
                            loss = compute_loss(pred, labels)
                            total_loss += loss.sum().item()
                            test_labels += labels
                            test_preds += pred
                    # å°†æ•°æ®ç©¿ä¼šCPU
                    test_labels = [i.cpu().numpy().tolist() for i in test_labels]
                    test_preds = [i.detach().cpu().numpy().tolist() for i in test_preds]
                    # è®¡ç®—è¯„ä»·æŒ‡æ ‡
                    best_threshold, best_auc = find_best_threshold(test_preds, test_labels)
                    test_preds = transform_preds(test_preds, best_threshold)
                    test_auc.append(best_auc)
                    test_acc.append(accuracy_score(y_pred=test_preds, y_true=test_labels))
                    test_f1.append(f1_score(y_pred=test_preds, y_true=test_labels))
                    test_loss.append(total_loss / len(test_labels))

                    # å¯»æ‰¾æœ€ä½³é˜ˆå€¼
                    all_best_threshold.append(best_threshold)

                    # æ•°æ®è®°å½•
                    log_dic = {"epoch": epoch, "train_loss": train_loss[-1], "train_auc": train_auc[-1],
                               "train_acc": train_acc[-1],
                               "train_f1": train_f1[-1],
                               "test_loss": test_loss[-1], "test_auc": test_auc[-1], "test_acc": test_acc[-1],
                               "test_f1": test_f1[-1], "time": time.time(),
                               "threshold": all_best_threshold[-1]}
                    print(str(log_dic))
                    df = pd.read_pickle(log_path)
                    df = df.append([log_dic])
                    df.reset_index(inplace=True, drop=True)
                    df.to_pickle(log_path)

                    # æ›´æ–°å­¦ä¹ ç‡
                    if test_auc[-1] < max(test_auc):
                        threshold += 1
                        dynamic_lr *= 0.6
                        optimizer = torch.optim.Adam(net.parameters(), lr=dynamic_lr)
                    else:
                        threshold = 0

                    # æ—©åœé˜²æ­¢è¿‡æ‹Ÿåˆ
                    if threshold >= patient:
                        index = np.argmax(np.array(test_auc))
                        print("epoch {}è¾¾åˆ°æœ€ä½³é˜ˆå€¼{} ".format(index, all_best_threshold[index]))
                        break
                all_best_auc[
                    "hiddens:{0}-layers:{1}-epoch:{2}".format(int(num_hiddens), int(num_layers), all_epoch)] = int(
                    np.max(test_auc) * 10000)
                print(all_best_auc)




    # test_list = [
    #     'æ‰‹æœºæ‹¿åˆ°æ‰‹ï¼Œæ„Ÿè§‰å¾ˆå¥½ã€‚å±å¹•ç»†è‡´ï¼Œæ‰‹æ„Ÿä¹Ÿä¸é”™ï¼Œè¿è¡Œé€Ÿåº¦è›®å¥½ã€‚è¿˜åœ¨è¯•ç”¨å½“ä¸­ã€‚è¿˜æœ‰ä¸€ç‚¹å……ç”µå¿«ç”¨ç”µå¿«ã€‚',
    #     'å€¼å¾—è´­ä¹°çš„äº§å“ï¼Œåšå·¥éå¸¸ç²¾ç»†ï¼Œæ¯”æƒ³è±¡ä¸­çš„å¥½ï¼Œæ¨èå¤§å®¶å°è¯•',
    #     'è¾£é¸¡æ‰‹æœºï¼Œç©äº†ä¸€ä¸ªæœˆçš„æ ·å­ï¼Œæ‰“ç‹è€…æœ‰æ—¶éƒ½æ‰å¸§ï¼Œæƒ³ç©æ¸¸æˆè¿˜æ˜¯åˆ«ä¹°å°ç±³ï¼ŒçœŸçš„ä¸è¡Œï¼Œæˆ‘åŒäº‹ä¹°çš„11æœ‰ä¸€å¹´ä¹Ÿä¸è¡Œï¼Œç”¨äº†å‡ ä¸ªå°ç±³æ‰‹æœºäº†ï¼ŒçœŸçš„ä¸æƒ³ç”¨äº†ï¼Œè¶Šæ¥è¶Šå·®ã€‚æœäº†ã€‚',
    #     "ç”¨äº†å¿«ä¸€ä¸ªæœˆï¼Œä»¥å‰ä¸æ˜¯å¾ˆäº†è§£å°ç±³æ‰‹æœºï¼Œä½†æ˜¯å°ç±³æ‰‹æœºæˆ‘ä¸ªäººè§‰å¾—è¿˜æ˜¯æ…å…¥ï¼Œå¯èƒ½æ˜¯æˆ‘è¿˜ä¸å¤ªä¼šç”¨å§ï¼Œä¸åˆ°ä¸€ä¸ªæœˆå·²ç»å¡äº†ä¸‰æ¬¡ğŸ˜§æˆ‘å¹³æ—¶ä¹Ÿä¸ç©æ¸¸æˆä¹ŸæŒ‰è¦æ±‚å‡çº§äº†ç³»ç»Ÿï¼Œæ„Ÿè§‰ç©ä¸æ˜ç™½äº†",
    #     "ä¸å’‹åœ°ï¼Œå‘çƒ«çš„å¾ˆï¼Œæ„Ÿè§‰ä¹°åæ‚”äº†",
    #     "è¿™ä¸ªæ‰‹æœºå¹¶æ²¡æœ‰æƒ³è±¡çš„é‚£ä¹ˆæµç•…ï¼Œæœ‰çš„æ—¶å€™å¾ˆå¡é¡¿ã€‚æœ‰ç‚¹å¤±æœ›ã€‚",
    #     'æˆ‘ä¹Ÿä¸çŸ¥é“æ€ä¹ˆå›äº‹ï¼Œè¿æˆ‘åŸæ¥çš„é‚£ä¸ªçº¢ç±³k40è¿˜ä¸å¦‚ï¼Œæ‰“åŸç¥æ€»æ˜¯å¡å¸§'
    # ]
    # for text in test_list:
    #     print(predict_sentiment(net, vocab, text, all_best_threshold[np.argmax(np.array(test_auc))]))

